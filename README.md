# Efficient-LLM-Few-Examples-Fine-Tuning
The long context consumed occupies useful space in a finite context window. Either re-training from scratch or fine-tuning on a large dataset is undesirable. Using context distillation to finetune on the prompt with a small dataset is expected to simplify the long context window problem with low cost.
